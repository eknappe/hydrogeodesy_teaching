{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa880b91",
   "metadata": {
    "id": "aa880b91"
   },
   "source": [
    "## HHW Lab 3 : Combinations Lab\n",
    "In this notebook, we'll combine what you learned from the previous notebooks and start comparing the GNSS time series to the hydrologic measurements we have. Feel free to refer back to the previous notebooks for a bit of helpful code.\n",
    "\n",
    "Goals for this notebook:\n",
    "* Explore different methods of comparison\n",
    "* Compare the GNSS vertical time series to multiple hydrologic datasets\n",
    "* Create a water balance to estimate hydrologic storage\n",
    "* Compare hydrologic storage to GNSS vertical time series \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b27e7",
   "metadata": {
    "id": "a22b27e7"
   },
   "source": [
    "For this, we've broken you up into groups and assigned different watersheds. At the end of the combinations lab, you and your team will put together a short presentation of your findings and how the vertical GNSS in your watershed compare to hydrologic measurements. \n",
    "\n",
    "Each watershed is classified by USGS as a HUC8 watershed and is roughly the same size. If you're interested in more info about hydrologic unit codes (HUC) or identifiers for other watersheds, you can get more info here: https://water.usgs.gov/GIS/huc.html.\n",
    "\n",
    "If you were assigned the Idaho watershed (Camas Creek, HUC8), you'll be working with GNSS station P350, Snotel station 830, and USGS gage 13141500. The area of this watershed is 1779.73 sq km. \n",
    "\n",
    "If you were assigned the Sierra watershed (Upper Yuba Feather), you'll be working with GNSS station P144, Snotel station 428, and USGS gauge 11413000. The area of this watershed is 3483.36 sq km.\n",
    "\n",
    "If you were assigned the CO watershed (Roaring Fork), you'll be working with GNSS station P728, Snotel station 737, and USGS gauge 09085000 - the most downstream gage that you'll use in part 4. USGS gage 09081000, gage 09076300, gage 09073400, gage 09073300, and gage 09072550 also exist along this river. The area of this watershed is 3767.35 sq km.\n",
    "\n",
    "Everyone will work on one snow-dominated watershed (above) AND the same rain-dominated watershed (Russian River). For the Russian River, you'll be working with GNSS station P192, and USGS gauge 11467000 - the most downstream gage that you'll use in part 4. USGS gage 11464000, gage 11463000, gage 11462500, and gage 11461000 also exist along this river. The area of this watershed is 3845.99 sq km."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264b112",
   "metadata": {
    "id": "b264b112"
   },
   "source": [
    "### PART 1: GNSS Data\n",
    "Let's start with the GNSS data. You'll import the GNSS data, the NTOL and NTAL data, which I've already combined for you and in called combo21.pnum.txt. Go ahead and remove the NTAL/NTOL signal and cut the timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca4e5b2",
   "metadata": {
    "id": "8ca4e5b2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GNSS_data/????_UNR.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29580\\3022556028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m gps = pd.read_csv(\"GNSS_data/????_UNR.txt\", header = None, \n\u001b[0m\u001b[0;32m     18\u001b[0m                    skiprows = 1, names = ['Date', 'east', 'north','vertical', 'e_std','n_std','v_std'])\n\u001b[0;32m     19\u001b[0m \u001b[0mgps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GNSS_data/????_UNR.txt'"
     ]
    }
   ],
   "source": [
    "##Import the libraries you'll likely need\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "##This is what you'll cut your time series to, GNSS and hydrologic\n",
    "startdate = pd.to_datetime('10/1/2008', format = '%m/%d/%Y')\n",
    "enddate = pd.to_datetime('9/29/2021', format = '%m/%d/%Y')\n",
    "\n",
    "##Import the libraries you'll likely need\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "gps = pd.read_csv(\"GNSS_data/????_UNR.txt\", header = None, \n",
    "                   skiprows = 1, names = ['Date', 'east', 'north','vertical', 'e_std','n_std','v_std'])\n",
    "gps.Date = pd.to_datetime(gps.Date)\n",
    "gps = gps.set_index('Date')\n",
    "gps = gps[startdate:enddate]\n",
    "\n",
    "##add in the atmospheric loading\n",
    "atmos = pd.read_csv(\"GNSS_data/????.txt\", skiprows = 1, header = None, \n",
    "                    names = ['a_n','a_e', 'a_v','Date'])\n",
    "#Convert to datetime\n",
    "atmos.Date = pd.to_datetime(atmos.Date)\n",
    "#Convert from m to mm  \n",
    "atmos.a_v = atmos.a_v * 1000\n",
    "atmos = atmos.set_index('Date')\n",
    "atmos = atmos.sort_values(by='Date')\n",
    "    \n",
    "full = pd.concat([gps3, atmos], axis=1)\n",
    "full['new_v'] = full.vertical - full.a_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7f8e1",
   "metadata": {
    "id": "0be7f8e1"
   },
   "source": [
    "Next, we'll fit and remove a linear trend. Just the linear trend since we want to keep the annual hydrologic signal in at this point but remove the long term tectonic signal. Again, I recommend plotting to make sure everything looks a-ok. If there is a large offset in your data, you can cut your time series to avoid it (or if you cruised through the GNSS notebook go ahead and fit and correct the offset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd76af",
   "metadata": {
    "id": "76fd76af"
   },
   "outputs": [],
   "source": [
    "full['t']=full.index.map(dt.datetime.toordinal)\n",
    "full = full.dropna()\n",
    "\n",
    "verticalseries = lin_remove(full.new_v, full.v_std, full.t) \n",
    "full.new_v.update(verticalseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b70eb9",
   "metadata": {
    "id": "83b70eb9"
   },
   "source": [
    "Go ahead and fit the harmonic to the data and calculate what months the min and max occur - you can just estimate. **Don't remove it!** We'll just use the timing information later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281b51a",
   "metadata": {
    "id": "4281b51a"
   },
   "outputs": [],
   "source": [
    "### Harmonic function\n",
    "# y(t) = y_0 +vt_i + SUM(from k=1) a_k*sin(w_k * t_i) + b_k*cos(w_k*t_i)\n",
    "# G*m = d style\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "# Ingest 1 component of the timse seris, omega, time in ordinal \n",
    "def harm_fit(tseries, omega, dtime, semi):\n",
    "    #Observations\n",
    "    dv = np.array(tseries)\n",
    "    if (semi == False): #just the annual\n",
    "        #G matrix\n",
    "        #Start with an empty matrix\n",
    "        Gv = np.zeros((len(dv), 4))\n",
    "        #Make the columns to be filled\n",
    "        c1v = np.ones((len(dv)))\n",
    "        c2v = np.array(dtime)\n",
    "        c3v = np.sin(c2v*omega)\n",
    "        c4v = np.cos(c2v*omega)\n",
    "        #Fill the G matrix\n",
    "        Gv[:,0] = c1v\n",
    "        Gv[:,1] = c2v\n",
    "        Gv[:,2] = c3v\n",
    "        Gv[:,3] = c4v\n",
    "        #Solve using Least Squares\n",
    "        mv = np.linalg.solve(np.dot(np.transpose(Gv), Gv),np.dot(np.transpose(Gv),dv))\n",
    "        # Full harmonic :\n",
    "        y_v = mv[0] + mv[1]*dtime + mv[2]*np.sin(omega*dtime) + mv[3]*np.cos(omega*dtime)\n",
    "    else: #annual + semi annual\n",
    "        omega2 = 2*omega\n",
    "        #G matrix\n",
    "        #Start with an empty matrix\n",
    "        G2 = np.zeros((len(dv), 6))\n",
    "        #Make the columns to be filled\n",
    "        c1 = np.ones((len(dv)))\n",
    "        c2 = np.array(dtime)\n",
    "        c3 = np.sin(c2*omega)\n",
    "        c4 = np.cos(c2*omega)\n",
    "        c5 = np.sin(c2*omega2)\n",
    "        c6 = np.cos(c2*omega2)\n",
    "        #Fill the G matrix\n",
    "        G2[:,0] = c1\n",
    "        G2[:,1] = c2\n",
    "        G2[:,2] = c3\n",
    "        G2[:,3] = c4\n",
    "        G2[:,4] = c5\n",
    "        G2[:,5] = c6\n",
    "        #Solve using Least Squares\n",
    "        m2 = np.linalg.solve(np.dot(np.transpose(G2), G2),np.dot(np.transpose(G2),dv))\n",
    "        # Full harmonic :\n",
    "        y_v = m2[0] + m2[1]*dtime + m2[2]*np.sin(omega*dtime) + m2[3]*np.cos(omega*dtime) + m2[4]*np.sin(omega2*dtime) +m2[5]*np.cos(omega2*dtime)\n",
    "    return y_v, m2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmfit, m = harm_fit(full.new_v, 2*np.pi/365.25, full.t, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate when the harmonic reaches its min and max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb6407",
   "metadata": {
    "id": "aadb6407"
   },
   "source": [
    "Earlier, we were focused on daily time series. One method that is often used in GNSS processing (for hydrology or for other purposes) is applying a temporal smoothing to the data. This bumps up the signal to noise ratio of the GNSS data. With pandas, a simple moving mean or median is easy to do. We'll be using the \"rolling function\": https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html\n",
    "\n",
    "As with everything, there are many ways to do the same thing. If you're feeling confident in your python skills you can go ahead and figure out how to write a weighted rolling mean or median, incorporating the daily uncertainties. \n",
    "\n",
    "Go ahead and mess around with a few different temporal lengths - plotting them over the actual vertical time series to see how they perform. Try a mean and a median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0acfb",
   "metadata": {
    "id": "bfa0acfb"
   },
   "outputs": [],
   "source": [
    "#plot your rolling means with the data until you determine the best one\n",
    "full['rmed'] = full.new_v.rolling(??).median()\n",
    "full['rmean'] = full.new_v.rolling(??).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2481be1",
   "metadata": {
    "id": "d2481be1"
   },
   "source": [
    "Great, now that you have a little feel for how the different options might work, we're going to be using a 30 day rolling median for the rest of this. The reason we are using a median over a mean is because a median is less sensitive to outliers. And even though you've removed most with your previous function there might be a few more that were missed. However, other scientists might prefer a mean. And if you feel strongly, then use a mean. Go ahead and fit a 30 day rolling median. For the rest of the notebook, we'll be using this 30 day rolling median so if I refer to the gps data this is what I'm talking about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405cab3",
   "metadata": {
    "id": "7405cab3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1288d54d",
   "metadata": {
    "id": "1288d54d"
   },
   "source": [
    "Before we dive into the hydrologic data, let's check the GNSS data by plotting it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a873dbd",
   "metadata": {
    "id": "5a873dbd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede478e8",
   "metadata": {
    "id": "ede478e8"
   },
   "source": [
    "**Q1: Based on the GNSS data, describe the hydrologic signals that you see. Be specific.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb98f8",
   "metadata": {
    "id": "f5eb98f8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23755ed",
   "metadata": {
    "id": "a23755ed"
   },
   "source": [
    "### PART 2: Hydrologic Data\n",
    "Now let's retrieve the hydrologic data from Hydroshare for rain and snow dominated sites. For each snow-dominated site, you should find SWE from the Snotel station and streamflow from the USGS gage. You'll also find precipitation and reference evapotranspiration (ET) cut to the boundaries of the watershed with each measurement in every pixel within the watershed summed into one daily value from the Gridded Surface Meteorological dataset (Gridmet). For the rain-dominated site, you should find streamflow from the USGS gage. You'll also find precipitation and reference evapotranspiration (ET) summed over the watershed from Gridmet. Go ahead and check out the Gridmet website to make sure it makes sense where those values come from: https://www.climatologylab.org/gridmet.html \n",
    "\n",
    "\n",
    "Read those datasets into python using pandas. Be sure to track the units of each measurement. Personally, I like to add the units to the end of the column name like precip_mm, for example.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad46a979",
   "metadata": {
    "id": "ad46a979"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "#Read in hydrologic data\n",
    "filename = 'CamasCreek_HydroData.csv'\n",
    "Camas = pd.read_csv(filename, header = [0])\n",
    "Camas['Date'] = pd.to_datetime(Camas['Date'], infer_datetime_format=True)\n",
    "Camas.set_index(Camas['Date'], inplace = True)\n",
    "Camas.dropna()\n",
    "\n",
    "filename = 'RoaringFork_HydroData.csv'\n",
    "RF = pd.read_csv(filename, header = [0])\n",
    "RF['Date'] = pd.to_datetime(RF['Date'], infer_datetime_format=True)\n",
    "RF.set_index(RF['Date'], inplace = True)\n",
    "RF.dropna()\n",
    "\n",
    "filename = 'YubaFeather_HydroData.csv'\n",
    "YF = pd.read_csv(filename, header = [0])\n",
    "YF['Date'] = pd.to_datetime(YF['Date'], infer_datetime_format=True)\n",
    "YF.set_index(YF['Date'], inplace = True)\n",
    "YF.dropna()\n",
    "\n",
    "filename = 'RussianRiver_HydroData.csv'\n",
    "RR = pd.read_csv(filename, header = [0])\n",
    "RR['Date'] = pd.to_datetime(RR['Date'], infer_datetime_format=True)\n",
    "RR.set_index(RR['Date'], inplace = True)\n",
    "RR.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f190d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliss\\AppData\\Local\\Temp\\ipykernel_29580\\1732074040.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Camas.wateryear[i] = Camas.year[i] + 1\n",
      "C:\\Users\\aliss\\AppData\\Local\\Temp\\ipykernel_29580\\1732074040.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  RF.wateryear[i] = RF.year[i] + 1\n",
      "C:\\Users\\aliss\\AppData\\Local\\Temp\\ipykernel_29580\\1732074040.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  YF.wateryear[i] = YF.year[i] + 1\n",
      "C:\\Users\\aliss\\AppData\\Local\\Temp\\ipykernel_29580\\1732074040.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  RR.wateryear[i] = RR.year[i] + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WY columns added\n"
     ]
    }
   ],
   "source": [
    "Camas['year'] = pd.DatetimeIndex(Camas['Date']).year\n",
    "Camas['month'] = pd.DatetimeIndex(Camas['Date']).month\n",
    "Camas['wateryear'] = Camas['year']\n",
    "    \n",
    "for i in range(len(Camas.Date)):\n",
    "    if Camas.month[i] > 9:\n",
    "        Camas.wateryear[i] = Camas.year[i] + 1\n",
    "\n",
    "RF['year'] = pd.DatetimeIndex(RF['Date']).year\n",
    "RF['month'] = pd.DatetimeIndex(RF['Date']).month\n",
    "RF['wateryear'] = RF['year']\n",
    "    \n",
    "for i in range(len(RF.Date)):\n",
    "    if RF.month[i] > 9:\n",
    "        RF.wateryear[i] = RF.year[i] + 1\n",
    "        \n",
    "YF['year'] = pd.DatetimeIndex(YF['Date']).year\n",
    "YF['month'] = pd.DatetimeIndex(YF['Date']).month\n",
    "YF['wateryear'] = YF['year']\n",
    "    \n",
    "for i in range(len(YF.Date)):\n",
    "    if YF.month[i] > 9:\n",
    "        YF.wateryear[i] = YF.year[i] + 1\n",
    "        \n",
    "RR['year'] = pd.DatetimeIndex(RR['Date']).year\n",
    "RR['month'] = pd.DatetimeIndex(RR['Date']).month\n",
    "RR['wateryear'] = RR['year']\n",
    "    \n",
    "for i in range(len(RR.Date)):\n",
    "    if RR.month[i] > 9:\n",
    "        RR.wateryear[i] = RR.year[i] + 1\n",
    "print('WY columns added')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NdHlCYzroyyZ",
   "metadata": {
    "id": "NdHlCYzroyyZ"
   },
   "source": [
    "**Q2: What is the resolution of gridmet? What data is available from gridmet?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53834344",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "g6K860B_pZfa",
   "metadata": {
    "id": "g6K860B_pZfa"
   },
   "source": [
    "**Q3: What is the citation for Gridmet data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02e3b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49184e04",
   "metadata": {
    "id": "49184e04"
   },
   "source": [
    "Go ahead and plot each time series to get an idea of how the datasets change over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd39349",
   "metadata": {
    "id": "cdd39349"
   },
   "outputs": [],
   "source": [
    "#Plot hydrologic time series from your watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df96ad",
   "metadata": {
    "id": "98df96ad"
   },
   "source": [
    "**Q4: Comment on any longer-term temporal changes that you see. Can you explain what may be causing these?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8c684",
   "metadata": {
    "id": "a8d8c684"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e8dc289",
   "metadata": {
    "id": "4e8dc289"
   },
   "source": [
    "**Q5: Describe the datasets across one full WY. Do you notice a relationship between the fluxes (as in, as x decreases y begins to increase)?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5da48",
   "metadata": {
    "id": "13f5da48"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2fdc46c",
   "metadata": {
    "id": "e2fdc46c"
   },
   "source": [
    "Find the max SWE and streamflow and the dates of those peaks for every WY in your snow-dominated site and find the date associated with the max value. \n",
    "\n",
    "df_max_Q = max(df.variable) \n",
    "df_max_Q_date = df[['variable']].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ec5b5",
   "metadata": {
    "id": "5e0ec5b5"
   },
   "outputs": [],
   "source": [
    "#Calculate max "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DKk7IwpFpYkL",
   "metadata": {
    "id": "DKk7IwpFpYkL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7290f32",
   "metadata": {
    "id": "b7290f32"
   },
   "source": [
    "**Q6: How do the SWE and streamflow max dates compare for your snow dominated watershed? What is the lag time between them (one max date - the other max date) and are those lags consistent across WYs within the whole time series?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa3d6c",
   "metadata": {
    "id": "39aa3d6c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16ab486",
   "metadata": {
    "id": "b16ab486"
   },
   "source": [
    "**Q7: How do the dates of max streamflow and precip compare? Do you expect the lag to be larger or smaller in rain-dominated watersheds and why do you think that?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53053fe4",
   "metadata": {
    "id": "53053fe4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d29372f",
   "metadata": {
    "id": "7d29372f"
   },
   "source": [
    "**Q8: For a few WYs, compare the timing of max streamflow between your snow and rain dominated watersheds. Go back to your earlier description of temporal changes (see Q4). Do these values support your earlier qualitative analysis?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa5fad",
   "metadata": {
    "id": "b4aa5fad"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b13d268",
   "metadata": {
    "id": "0b13d268"
   },
   "source": [
    "Go ahead and compare the locations of all of the watersheds provided by creating a map using pyGMT. Feel free to refer back to the hydro notebook section 5 for helpful code to create this map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c432b1",
   "metadata": {
    "id": "a7c432b1"
   },
   "outputs": [],
   "source": [
    "#Create a map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a707f64",
   "metadata": {
    "id": "6a707f64"
   },
   "source": [
    "**Q9: Based on the location of these sites, can you make any generalizations about the distribution of water across the western US?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed309cb8",
   "metadata": {
    "id": "ed309cb8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a153a381",
   "metadata": {
    "id": "a153a381"
   },
   "source": [
    "### PART 3: GNSS and Hydrologic Comparisons\n",
    "Let's dive into some comparisions of GNSS and hydrologic time series. We'll begin with the basics. Go ahead and plot the GPS vertical signal and and the Snotel SWE observation on the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db6c38",
   "metadata": {
    "id": "19db6c38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8071981e",
   "metadata": {
    "id": "8071981e"
   },
   "source": [
    "**Q10: Describe how the two signals compare. Be detailed here. Go ahead and zoom in temporally as well as comparing the big picture (e.g., specific water years vs the longer term trends)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5909670",
   "metadata": {
    "id": "a5909670"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a84fe8c0",
   "metadata": {
    "id": "a84fe8c0"
   },
   "source": [
    "Now, let's quantatively compare the two signals. We'll be using a Pearson's correlation coefficient (sometimes referred to pearson's R) which is a measure of linear correlation. For a more detailed explanation: https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/. \n",
    "\n",
    "It's a standard stastical test to test how two datasets relate (linearly in this case). This is just one option - there are so many statistical tests that can be used. Python has a ton of options to perform a pearson's correlation -- numpy has corrcoeff, scipy stats has pearsons r, or you can write your own if you feel zesty. \n",
    "\n",
    "eg:\n",
    "\n",
    "*variable = df.variable.corr(df2.variable2)*\n",
    "\n",
    "or\n",
    "\n",
    "*np.corrcoeff(df1.variable1, df2.variable2)*\n",
    "\n",
    "Go head and calculate the pearson's R for the gps vertical signal and snotel swe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb34f6",
   "metadata": {
    "id": "c1bb34f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c2c5b11",
   "metadata": {
    "id": "9c2c5b11"
   },
   "source": [
    "**Q11: Does this value make sense? If values closer to -1 and 1 indicate stronger correlation, how does this compare? What might make this correlation stronger?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8504c7",
   "metadata": {
    "id": "ee8504c7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a818fe6",
   "metadata": {
    "id": "9a818fe6"
   },
   "source": [
    "Another common statistical test is a cross correlation which compares two time series at a series of lagged/shifted positions. In other words, it's a measure of similiarity between two time series as a function of the lag of one series relative to the other. Here's a link with a more detailed explanation and a visual example: https://robosub.eecs.wsu.edu/wiki/ee/hydrophones/start or https://en.wikipedia.org/wiki/Cross-correlation. A cross correlation allows us to see if there is a time delay between the two series. Before reading the rest of the notebook, I want you to think about how this statistical test could be used in the context of hydrogeodesy, hydrology or geodesy in general. \n",
    "\n",
    "**Q12: Brainstorm how a cross correlation could be used? What about other uses for a Pearsons' correlation? Any other statistical test that you think could be useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c07c27",
   "metadata": {
    "id": "b0c07c27"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf6e9600",
   "metadata": {
    "id": "bf6e9600"
   },
   "source": [
    "One thing you need to be careful of when running cross correlations is that it typically removes data on the end, shortening the time series that is being shifted, resulting in a smaller amount of data being compared as you increase the lag. If you're time series is long, this typically isn't a huge deal but remains something to be aware of. There are also functions that will loop the time series back on itself, as well.\n",
    "\n",
    "As when performing any stastical test, you should be thinking about what you're trying to look into because **correlation does not equal causation**. I love this website about spurious correlations: https://www.tylervigen.com/spurious-correlations. It's always good to keep in mind the possible physical mechanism that could lead to correlation. Above, we just correlated SWE to GPS vertical postion. \n",
    "\n",
    "**Q13: What mechanism would cause SWE to be correlated to vertical position? (Not a trick question)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd99c7",
   "metadata": {
    "id": "e0cd99c7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700556bd",
   "metadata": {
    "id": "700556bd"
   },
   "source": [
    "For cross correlations you should be thinking the same thing. The sanity check of: does this make sense? It will also help you set up the problem. When you are using a cross correlation function you can define the max time delay you want to investigate, e.g., 10 days, 20 days, the whole time series length. You want to keep in mind what the mechanism could be that would produce that time delay. Unless there is a reason for it, you probably don't want to just automatically use the full time series length since that is a lot of calculations. \n",
    "\n",
    "Let's start with correlating SWE and GPS vertical position. \n",
    "\n",
    "**Q14: What do you think is the time delay that would produce the highest correlation? What do you think is the maximum time lag you should use? Give me justification for this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348b162",
   "metadata": {
    "id": "b348b162"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a40f132",
   "metadata": {
    "id": "1a40f132"
   },
   "source": [
    "Within python there are many ways to perform a cross correlation including a few built in functions such as statsmodels ccf, or matplotlibs xcorr, or writing a small loop using one of the correlation coefficient functions (of which there are tons already written online). Go ahead and perform a cross correlation. Also, don't forget that you've applied a 30 day smoothing that can sometimes cause signals to be \"shmeared\" into the surrounding days. If you think this might be what is happening with your specific station go ahead and run the cross correlation without the smoothing too. \n",
    "\n",
    "Before doing this, you'll want to merge the GPS dataframe and the hydro dataset. \n",
    "\n",
    "*combined_df = pd.merge([df1,df2], axis = 1)*\n",
    "\n",
    "And, also remove any NaNs using df = df.dropna()\n",
    "\n",
    "\n",
    "eg\n",
    "\n",
    "*plt.xcorr(df.variable, df.variable2, maxlags=numoflags)*\n",
    "\n",
    "\n",
    "import statsmodels.tsa.stattools as smt\n",
    "\n",
    "*smt.ccf(df.variable1, df.variable2)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065338c",
   "metadata": {
    "id": "6065338c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0047191",
   "metadata": {
    "id": "e0047191"
   },
   "source": [
    "**Q15: Are you correct? Does this make sense? Feel free to ask a neighbor or us if you're confused.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8ec6e",
   "metadata": {
    "id": "00e8ec6e"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79b1b086",
   "metadata": {
    "id": "79b1b086"
   },
   "source": [
    "While that might not have been the most interesting cross correlation to perform sometimes it's good to check if the data is behaving how you might expect (or maybe you didn't expect that, which is totally fine!, because now you've learned something. In my opinion, noodling around with the data is the best way to gain an understanding of what is going on). \n",
    "\n",
    "You don't have any snow measurements to compare to GNSS in your rain-dominated watershed (Russian River). Instead, compare daily precipitation to GNSS vertical deflection and move through all the same analysis that you just did in your snow-dominated watersheds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed77d6c",
   "metadata": {
    "id": "2ed77d6c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a50f9011",
   "metadata": {
    "id": "a50f9011"
   },
   "source": [
    "**Q16: How do your correlations of precip and vertical displacement compare to those of SWE and vertical displacement? Does this fit your expectation? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f553a",
   "metadata": {
    "id": "1f6f553a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d9cb8d9",
   "metadata": {
    "id": "5d9cb8d9"
   },
   "source": [
    "**Q17: How does the time delay compare between rain and snow dominated watersheds? Can you explain why you are seeing these time delays?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401b67d",
   "metadata": {
    "id": "d401b67d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c54ad07",
   "metadata": {
    "id": "6c54ad07"
   },
   "source": [
    "So far, we have compared vertical deflection to the main inputs to our system. Let's move onto another cross correlation and go ahead and compare to a well-known output instead. We're going to compare the GPS vertical position to the streamflow in your watershed. If you have more then 1 gage in your system, compare it to both. But before you begin:\n",
    "\n",
    "**Q18: What do you think you should set as your maximum lag?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a979f",
   "metadata": {
    "id": "217a979f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a846538",
   "metadata": {
    "id": "7a846538"
   },
   "source": [
    "Great, go ahead and perform the cross correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eff61d",
   "metadata": {
    "id": "b9eff61d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52fa1fe3",
   "metadata": {
    "id": "52fa1fe3"
   },
   "source": [
    "**Q19: As always, does this make sense? Explain what might be the reason for this? If you had more then one gage, were they the same? Does your answer make sense?**  If you are starting to feel like this is tedious, I'm trying to force you all into good habits. Sometimes its easy to just run all these statistical tests or compute these mathematical models and trust whatever comes out of them but I want all of you to sanity check the answers - in this short course but also moving forward in whatever research you end up doing. Ok, I'll get off my soap box. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848cc93",
   "metadata": {
    "id": "3848cc93"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd7e01a",
   "metadata": {
    "id": "3fd7e01a"
   },
   "source": [
    "### PART 4: Estimating hydrologic storage\n",
    "You just saw that GNSS vertical displacements are somewhat related to system inputs and outputs. Now let's explore the relationship between those with a simple mass balance. We're going to estimate hydrologic storage using the following equation:\n",
    "**dS/dt = Inputs - Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe692fd",
   "metadata": {
    "id": "6fe692fd"
   },
   "source": [
    "**Q20: What are the inputs to our system?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbac06d",
   "metadata": {
    "id": "4fbac06d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c50f8b4",
   "metadata": {
    "id": "4c50f8b4"
   },
   "source": [
    "**Q21: What about the outputs? Keep in mind that there is one more output we have not explored yet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e04281",
   "metadata": {
    "id": "91e04281"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "D62U-whNvDMZ",
   "metadata": {
    "id": "D62U-whNvDMZ"
   },
   "source": [
    "**Q22: Write out your new dS/dt equation for each watershed. Keep in mind that the change in storage from the day before (t-1) will impact the daily storage for the current day (t).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NKH5yFVWvMty",
   "metadata": {
    "id": "NKH5yFVWvMty"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01cda0b",
   "metadata": {
    "id": "d01cda0b"
   },
   "source": [
    "We also need to think about defining the boundaries of our system. We will use the watershed as our lateral bounds. The data provided to you has been cut to the watershed. \n",
    "\n",
    "What about the vertical bounds? For this simple estimate, we are excluding groundwater flow. There is no specific depth of the vertical boundary, but rather the water balance does not extend into the saturated zone of groundwater. We will not account for groundwater loss or gain because those constraints can be quite a bit more complex and challenging to find, especially in mountain systems.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ArYukc8-xk_o",
   "metadata": {
    "id": "ArYukc8-xk_o"
   },
   "source": [
    "Another important thing to think about when estimating hydrologic storage is the units of our inputs and outputs. For your water balance, you'll want everything in terms of cubic meters/day. Remember back to our earlier discussion of daily fluxes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f8ee5",
   "metadata": {},
   "source": [
    "**Q23: What are the units of your hydrologic datasets? Streamflow? Precipitation? ET? SWE?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe8b66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3fe0ce",
   "metadata": {},
   "source": [
    "**Q24: What do you need to convert depth to volume? Hint: For precip and ET, remember back to Q2 about the resolution of Gridmet because we actually summed the measurement from each pixel. For SWE, we are making a very large assumption by assuming the SWE measured at our Snotel station is representative of the whole watershed so look back at our initial description of the watersheds.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368d6ac",
   "metadata": {},
   "source": [
    "Because we are calculating a daily change in storage, go ahead and convert everything to cubic meters/day. Hint: first, make sure you get everything into meters or m3/sec for streamflow. Then convert from depth to volume and account for the time component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WlooEDJixxnq",
   "metadata": {
    "id": "WlooEDJixxnq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "YHiXkoudwVNO",
   "metadata": {
    "id": "YHiXkoudwVNO"
   },
   "source": [
    "Now, go ahead and estimate the daily change in hydrologic storage for your snow-dominated and rain-dominated watersheds using the datasets provided. Keep in mind that your equations for dS/dt will be slightly different for the two types of watersheds. Also, remember that snow-dominated watersheds can still be influenced by rainfall. Because SWE measures the volume and the mass  of the snowpack (to calculate density), liquid precipitation should be accounted for by SWE measurements during the snowpack; however, it will need to be addressed in the absense of SWE. \n",
    "\n",
    "A few other important hints: you will have to set an arbitrary start value for dS/dt for this calculation. Because there are many different scenarios to consider (if t == 0, if t > 0), you'll need to use a for loop for this calculation. In the snow-dominated waterhsed, if and elif statments will be needed to account for precip in the absence of SWE. Feel free to discuss with neighbors and us! If you are not familiar with for loops and if statements, we're happy to help you through this. It can be a bit frustrating at first. \n",
    "\n",
    "The avg2_v is the vertical of the GPS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e356a01",
   "metadata": {
    "id": "0e356a01"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/m6llqxgd5t5_2st_71j81f1r0000gn/T/ipykernel_14944/3981898846.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg2_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStorage_m3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# df.Storage_m3[i] = df.Storage_m3[i-1] + df.precip_m3[i] + df.swe_m3[i] - df.ET_m3[i] - df.Q_m3[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(df.avg2_v)):\n",
    "    if i == 0 :\n",
    "        df.Storage_m3[i] = 0\n",
    "    else :\n",
    "        # df.Storage_m3[i] = df.Storage_m3[i-1] + df.precip_m3[i] + df.swe_m3[i] - df.ET_m3[i] - df.Q_m3[i]\n",
    "        if df.swe_m3[i] > 0 :\n",
    "            df.Storage_m3[i] = df.Storage_m3[i-1] + df.swe_m3[i] - df.ET_m3[i] - df.Q_m3[i]\n",
    "        elif df.swe_m3[i] == 0 :\n",
    "            df.Storage_m3[i] = df.Storage_m3[i-1] + df.precip_m3[i] - df.ET_m3[i] - df.Q_m3[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253bed4",
   "metadata": {
    "id": "a253bed4"
   },
   "source": [
    "Go ahead and plot up your time series of hydrologic storage.\n",
    "\n",
    "**Q25: Do your estimates of hydrologic storage follow the temporal trends that you expected to see? Explain.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7852c",
   "metadata": {
    "id": "a2d7852c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d47d5c7",
   "metadata": {},
   "source": [
    "It is very likely that they are trending strongly in one direction. What do you think might be causing this? \n",
    "\n",
    "\n",
    "Hopefully you guessed that reference ET is swamping the other fluxes. Let's chat briefly about ET. Remember when we were learning all about how the met station instruments worked and what they measured. You should have noticed that the Climavue (and nearly all met stations) are equipped to measure everything needed to calculate ET (typically using the Penman-Monteith equation but there are lots of options. Feel free to dig into those, if you're interested.) Unfortunately, actual ET (what is actually evaporated and transpired from the surface) is difficult to constrain at scales larger than point measurements because it depends on the met conditions, the amount of available water, and the surface type (i.e., plant type or water body). Therefore, we rely on estimates of what could evaporate and transpire at larger scales - either potential ET or reference ET. In this case, Gridmet calculates reference ET assuming that the land surface is covered with grass (with a fairly well constrained transpiration rate). That leads to an overestimation of ET, but at least it gives us somewhere to start from and gives a pretty good idea of how ET should change temporally.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8e09e",
   "metadata": {},
   "source": [
    "Brainstorm ideas to address this. Personally, I tend to set a constraint on my water balance that does not allow dS/dt to drop below zero. If this is what you are seeing in your time series of dS/dt, go ahead and address this issue by adding one more if statement to your for loop that calculates dS/dt (if S < 0) -- we did it already, see below. Compare your watershed storage again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.avg2_v)):\n",
    "    if i == 0 :\n",
    "        df.Storage_m3[i] = 0\n",
    "    else :\n",
    "        # df.Storage_m3[i] = df.Storage_m3[i-1] + df.precip_m3[i] + df.swe_m3[i] - df.ET_m3[i] - df.Q_m3[i]\n",
    "        if df.swe_m3[i] > 0 :\n",
    "            df.Storage_m3[i] = df.Storage_m3[i-1] + df.swe_m3[i] - df.ET_m3[i] - df.Q_m3[i]\n",
    "        elif df.swe_m3[i] == 0 :\n",
    "            df.Storage_m3[i] = df.Storage_m3[i-1] + df.precip_m3[i] - df.ET_m3[i] - df.Q_m3[i]\n",
    "    if df.Storage_m3[i] < 0 :\n",
    "        df.Storage_m3[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501682a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf030616",
   "metadata": {
    "id": "bf030616"
   },
   "source": [
    "**Q26: Now, do you see any initial differences in the estimated hydrologic storage of your two watersheds?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8HKpSTGtqGO",
   "metadata": {
    "id": "c8HKpSTGtqGO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab0294cd",
   "metadata": {
    "id": "ab0294cd"
   },
   "source": [
    "Finally, let's see how our estimates of hydrologic storage compare to GNSS vertical displacement. I find it helpful to compute relative values of storage and displacement using df.variable/max(df.variable) and compare the relative magnitudes rather than the absolute magnitudes because of the large difference in absolute values. Go ahead and calculate the relative magnitude of dS/dt and vertical displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423578ae",
   "metadata": {
    "id": "423578ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6Uy3DFhk0KFl",
   "metadata": {
    "id": "6Uy3DFhk0KFl"
   },
   "source": [
    "Now compute pearson correlation coefficients like you did earlier. Plot up your time series using shared x axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hIl66vT70LlM",
   "metadata": {
    "id": "hIl66vT70LlM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94e07fe1",
   "metadata": {
    "id": "94e07fe1"
   },
   "source": [
    "**Q27: How does the relationship between estimated hydrologic storage and vertical displacement compare to the relationships between your hydrologic fluxes (i.e., inputs and outputs) and vertical displacement? Is this what you expected? Is it the same across both watersheds?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26dccc7",
   "metadata": {
    "id": "f26dccc7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Atsr_ZKg0PEb",
   "metadata": {
    "id": "Atsr_ZKg0PEb"
   },
   "source": [
    "**Q28: Is there a time lag? Go ahead and compute the cross correlation between storage and vertical component of GNSS - still using the relative magnitudes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aZ4-MNT10cbN",
   "metadata": {
    "id": "aZ4-MNT10cbN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "025397ba",
   "metadata": {},
   "source": [
    "**Be sure to keep in mind the assumptions that went into our hydrologic storage estimates. Think about how they could impact the correlations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EuLdvtX20ogE",
   "metadata": {
    "id": "EuLdvtX20ogE"
   },
   "source": [
    "### PART 5: Putting it all together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Ny7aT3s0uID",
   "metadata": {
    "id": "4Ny7aT3s0uID"
   },
   "source": [
    "Now, as a group, we want you to put a little presentation of your findings together for the class since you all had different watersheds. At a minimum, we want a map of both of your watersheds with the location of the GNSS station, Snotel station, and USGS stream gages and any other relevant things. Also, some of your interesting findings (but make sure you include the calculated storage vs vertical GNSS). We want a few minute presentation from each group 5-10 minutes (short and stress-free). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ec2c1",
   "metadata": {
    "id": "d51ec2c1"
   },
   "source": [
    "### PART 6: Additional components for our speedy coders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hR9pWlG51sai",
   "metadata": {
    "id": "hR9pWlG51sai"
   },
   "source": [
    "Did you finish and still have a ton of time left over? If yes, continue on. If not, no worries you did everything we wanted you to get through. \n",
    "\n",
    "\n",
    "Alrighty, speedy coders. You have a few options and you can do either or both, depending on your interest. \n",
    "\n",
    "**OPTION 1**: Check out the LoadDef notebooks. LoadDef ingests load grids and calculates the response due to that load - e.g., it predicts the response at a given pixel due to a load (among other things). There are many applications relevant to this workshop including but not limited to: predicting surface displacements at GNSS stations due to GRACE  observations OR predicitng surface displacements due to a hydrologic model etc etc. It's currently a forward model program (though inverse modeling is in the works as well). Here's the full paper: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018EA000462 \n",
    "\n",
    "The documentation for LoadDef is great and Hilary has provided some notebooks for anyone that wants to get started with the program. If this is of interest to you, go ahead and check those notebooks out. Later today, Matthew will also be briefly talking about LoadDef and inversions. \n",
    "\n",
    "**OPTION 2:** We talked very briefly about increasing signal to noise ratios. Another way to increase signal to noise ratios is to use multiple stations and combine the data - creating a network mean or median. If this is of interest to you, continue on. But honestly, LoadDef is way more interesting, in my opinion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pxuWC0d4Ar3",
   "metadata": {
    "id": "3pxuWC0d4Ar3"
   },
   "source": [
    "Ok, option 2ers. I hope you've already done the LoadDef tutorial because I do honestly think it's way cooler. But here's what you'll do for option 2. Since you're advance coders you'll get way less prompting in this but feel free to ask us questions if you need help. \n",
    "\n",
    "Go ahead and pick a watershed from this notebook. Locate your station on the UNR NGL map view: http://geodesy.unr.edu/NGLStationPages/gpsnetmap/GPSNetMap.html \n",
    "\n",
    "Thinking about your watershed boundary, go ahead and pick some stations within or nearby your watershed boundary (at least 2 more additional stations). I typically like 10 + but for this notebook that might just be a lot unless you write a nifty little for loop (hint hint). With your additional stations in hand, go ahead and download the data within the \"common mode\" folder. These stations have already been corrected for NTAL & NTOL. I suggest plotting each of your new stations to make sure there is no anomalous data or an unaccounted for offset. There are a variety of ways to calculate a network mean (or common mode) and they can be used in a variety of ways. For this purpose, we'll calculate a network mean or network median. Using all your stations, calculate the daily mean or median position. (Pandas makes this easy if you use datetime like we've been suggesting). I also like to plot this mean/median over all the stations' positions to see if it makes sense and is doing what I think it is. Once you have your network mean, compare it to the storage estimate for that watershed. And then go through the questions again - did it improve your estimates? Worsen it? Can you think of why or why this happened? Feel free to include this in your presentation if you made it this far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dk58zxI33-_X",
   "metadata": {
    "id": "Dk58zxI33-_X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "BHzSYuR753Kp",
   "metadata": {
    "id": "BHzSYuR753Kp"
   },
   "source": [
    "There are a bunch of ways to calculate a common mode. If you're interested in estimating the common mode to filter out longer wavelength signals this is one option (calculating the network mean for a much larger area than the one you are interested in and removing it from the time series) or using a statistical approach such as a PCA or ICA. \n",
    "\n",
    "If you're interested in common mode and other applications in geodesy check out some of these papers: https://link.springer.com/article/10.1007/s00190-020-01466-5 OR https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/97JB01378 OR https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005JB003806 and here's a plug for my old paper that uses a similar method to what I just did. Making a case for using a network mean to fill in gaps in GRACE data: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2018WR023289 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a23755ed",
    "a153a381",
    "3fd7e01a"
   ],
   "name": "HHW3_Combo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
